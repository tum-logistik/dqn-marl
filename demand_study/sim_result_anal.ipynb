{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # <--- This is important for 3d plotting \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from environment.TheoreticalMarket import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sim_results/market-marl-nash-3-05-07-2022-00-17-5840_results.pkl\"\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    res = pkl.load(f)\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "data_losses = {\"Q Function Loss\": res.losses, \"Epsilon Function Loss\": res.losses_eps, \"Nash Net Loss\": res.losses_nash}\n",
    "data_losses_df = pd.DataFrame(data=data_losses)\n",
    "# sns.lineplot(data=data_losses_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.marl_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "ax = data_losses_df.plot(y=\"Q Function Loss\", legend=False, figsize=(8, 5))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "data_losses_df.plot(y=\"Nash Net Loss\", ax=ax2, legend=False, color=\"orange\")\n",
    "\n",
    "# data_losses_df.plot(y=\"Epsilon Function Loss\", ax=ax2, legend=False, color=\"red\")\n",
    "ax.figure.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# sns.lineplot(data=res.losses, color=\"b\", legend=\"auto\")\n",
    "# ax2 = plt.twinx()\n",
    "# sns.lineplot(data=res.losses_eps, color=\"g\", ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w=3):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "all_rewards = res.episode_rewards\n",
    "agent_ind = 0\n",
    "x = res.episode_rewards[:, -1, agent_ind]\n",
    "smoothed_episode_rewards = moving_average(x)\n",
    "\n",
    "# episode rewards all agents\n",
    "\n",
    "sns.set()\n",
    "x = np.arange(len(all_rewards))\n",
    "\n",
    "# plt.plot(x, avg_ag_actions, 'b-', label='Average Agent Price')\n",
    "# plt.plot(x, ref_prices, 'r--', label='Ref. Price')\n",
    "plt.plot(x[2:], smoothed_episode_rewards, 'r', label='Rewards per agent')\n",
    "plt.plot(x, res.avg_epoch_rewards_agent, 'b', label='Reward agent of interest')\n",
    "\n",
    "\n",
    "plt.legend(title='Pricing actions and Nash Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards\n",
    "\n",
    "data_rewards = {\"Average Agent Reward\": res.avg_epoch_rewards, \"Average Reward for Agent 0\": res.avg_epoch_rewards_agent}\n",
    "data_rewards_df = pd.DataFrame(data=data_rewards)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9, 5)})\n",
    "sns.lineplot(data=data_rewards_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Theoretical Data\n",
    "# tm = TheoreticalMarket(beta0 = 30, beta1 = -1.1, beta2 = -2, a = 0.1, ref_p = 1.5)\n",
    "# beta0 = 25\n",
    "# beta1 = -0.9\n",
    "# beta2 = -1.1\n",
    "# a = 0.1\n",
    "\n",
    "# Market 2\n",
    "# beta0 = 25\n",
    "# beta1 = -0.6\n",
    "# beta2 = -6.1\n",
    "# a = 0.1\n",
    "\n",
    "# Market 3\n",
    "beta0 = 15\n",
    "beta1 = -1.05\n",
    "beta2 = -3.1\n",
    "a = 0.1\n",
    "\n",
    "# Config 4\n",
    "# beta0 = 13\n",
    "# beta1 = -5.05\n",
    "# beta2 = -2.1\n",
    "# a = 0.2\n",
    "\n",
    "# Config 5\n",
    "# beta0 = 27\n",
    "# beta1 = -3.05\n",
    "# beta2 = -1.1\n",
    "# a = 0.2\n",
    "\n",
    "\n",
    "\n",
    "# Simulation Data\n",
    "ep = 31\n",
    "# ref_prices = res.state_tracker_epoch[ep]\n",
    "ref_prices = res.state_tracker[ep]\n",
    "\n",
    "episode_rewards = res.episode_rewards[ep]\n",
    "\n",
    "joint_actions = res.episode_actions\n",
    "episode_joint_actions = joint_actions[ep]\n",
    "\n",
    "prices = np.array([get_nash_eps_curve(r, beta0, beta1, beta2, a)[0] for r in ref_prices])\n",
    "devs = np.array([get_nash_eps_curve(r, beta0, beta1, beta2, a)[1] for r in ref_prices])\n",
    "demand = np.array([get_nash_eps_curve(r, beta0, beta1, beta2, a)[2] for r in ref_prices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = ref_prices\n",
    "x = prices[0]\n",
    "X, Y = np.meshgrid(x, y) \n",
    "z = devs\n",
    "\n",
    "# min_eps_price, max_eps_price = get_eps0_range(9.9, Y, X, z)\n",
    "\n",
    "nash_eps_bounds = [get_eps0_range(ref_price, Y, X, z) for ref_price in ref_prices ]\n",
    "nash_lower_bound =  [v[0] for v in nash_eps_bounds]\n",
    "nash_upper_bound =  [v[1] for v in nash_eps_bounds]\n",
    "\n",
    "# Episode rewards \n",
    "\n",
    "avg_ag_rewards = episode_rewards.mean(axis = 1)\n",
    "avg_ag_actions= episode_joint_actions.mean(axis = 1)\n",
    "agent_interest_action = episode_joint_actions[:, 1]\n",
    "\n",
    "sns.set()\n",
    "x = np.arange(len(avg_ag_actions))\n",
    "\n",
    "plt.plot(x, avg_ag_actions, '--', label='Average Agent Price')\n",
    "# plt.plot(x, ref_prices, 'r--', label='Ref. Price')\n",
    "# plt.plot(x, avg_ag_rewards, 'r--', label='Ref. Price')\n",
    "plt.plot(x, agent_interest_action, 'o--', label='Agent 0 Price')\n",
    "\n",
    "\n",
    "\n",
    "plt.fill_between(x, nash_upper_bound, nash_lower_bound, color='b', alpha=0.2)\n",
    "\n",
    "\n",
    "plt.legend(title='Pricing actions and Nash Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "computed_ref_price = np.mean(res.episode_actions[:,-1, :], axis =1)\n",
    "computed_avg_action = shift(computed_ref_price, -1, cval=computed_ref_price[-1])\n",
    "computed_ref_price_pair = np.array([computed_ref_price, computed_avg_action]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_eps_bounds_all_ep = [get_eps0_range(ref_price, Y, X, z) for ref_price in computed_ref_price]\n",
    "\n",
    "nash_lower_bound_all_ep =  [v[0] for v in nash_eps_bounds_all_ep]\n",
    "nash_upper_bound_all_ep =  [v[1] for v in nash_eps_bounds_all_ep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def est_revenue_from_refprice(crp):\n",
    "    demand = beta0 + beta1*crp[1] + beta2*(crp[1] - crp[0])\n",
    "    return demand * crp[1]\n",
    "\n",
    "nash_lower_bound_ref_pair = copy.deepcopy(computed_ref_price_pair)\n",
    "nash_upper_bound_ref_pair = copy.deepcopy(computed_ref_price_pair)\n",
    "nash_lower_bound_ref_pair[:, 1] = nash_lower_bound_all_ep\n",
    "nash_upper_bound_ref_pair[:, 1] = nash_upper_bound_all_ep\n",
    "nash_lower_bound_all_rev_ep =  [est_revenue_from_refprice(v)/3 for v in nash_lower_bound_ref_pair]\n",
    "nash_upper_bound_all_rev_ep =  [est_revenue_from_refprice(v)/3 for v in nash_upper_bound_ref_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "x = np.arange(len(nash_lower_bound_all_ep))\n",
    "\n",
    "plt.fill_between(x, nash_lower_bound_all_rev_ep, nash_upper_bound_all_rev_ep, color='b', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set()\n",
    "x = np.arange(len(nash_lower_bound_all_ep))\n",
    "\n",
    "plt.plot(x, res.avg_epoch_rewards, '--', label='Average Episode Rewards')\n",
    "plt.plot(x, res.avg_epoch_rewards_agent, '--', label='Reward Agent 0')\n",
    "# plt.plot(x, ref_prices, 'r--', label='Ref. Price')\n",
    "# plt.plot(x, avg_ag_rewards, 'r--', label='Ref. Price')\n",
    "# plt.plot(x, agent_interest_action, 'o--', label='Agent 0 Price')\n",
    "\n",
    "plt.fill_between(x, nash_lower_bound_all_ep, nash_upper_bound_all_ep, color='b', alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the solid blue line in inside the fill, there is no incentive to deviate, otherwise, there is incentive, and the agents should try to undercut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Ref Prices\": ref_prices, \n",
    "nash_bound_per_run = {\"Nash Lower Bound\": nash_lower_bound, \n",
    "    \"Nash Upper Bound\": nash_upper_bound, \n",
    "    \"Avg. Agent Rewards\": avg_ag_rewards, \n",
    "    \"Avg. Agent Actions\": avg_ag_actions}\n",
    "nash_bound_per_run_df = pd.DataFrame(data=nash_bound_per_run)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9, 5)})\n",
    "sns.lineplot(data=nash_bound_per_run_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_bound_per_run_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.episode_rewards[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_key = [0.0, 0.0, 0.0, 3.0]\n",
    "\n",
    "res.sna_policy_dict_iter[repr(state_key)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = 0\n",
    "agent_id = 0\n",
    "for x in range(10):\n",
    "    p = res.sna_policy_dict_iter[repr(state_key)][agent_id][x]\n",
    "    if p > max_p:\n",
    "        max_p = p\n",
    "        max_x = x\n",
    "    print(p)\n",
    "print(max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = 0\n",
    "agent_id = 1\n",
    "for x in range(10):\n",
    "    p = res.sna_policy_dict_iter[repr(state_key)][agent_id][x]\n",
    "    if p > max_p:\n",
    "        max_p = p\n",
    "        max_x = x\n",
    "    print(p)\n",
    "print(max_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = 0\n",
    "agent_id = 2\n",
    "for x in range(10):\n",
    "    p = res.sna_policy_dict_iter[repr(state_key)][agent_id][x]\n",
    "    if p > max_p:\n",
    "        max_p = p\n",
    "        max_x = x\n",
    "    print(p)\n",
    "print(max_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.vis-env3': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "520f41434f2cbf0f8df5fa7e2214f46b7c27d6834ab088b2670aeddbed0d24e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
